---
title: "Visualization of Model Predicting Cumulative Utterances (Group by Numbers of UT)"
author: "Chunyen"
date: "2024/3/13"
output: 
  pdf_document:
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
header-includes:
  - \usepackage{fontspec}
  - \setmainfont{Noto Sans CJK TC}
---

# Introduction

視覺化逐句累積輸入時模型預測的表現。

依照各篇對話的語句數分組。

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r include=FALSE}
# import libraries
library(tidyverse)
library(ggplot2)
library(yardstick)
```

```{r}
# set working directory
setwd("/home/quantlab/2024_ChunyenThesis/Bert")
```

```{r include=FALSE}
# Fix the Mandarin font issue
library(showtext)

# 添加字體
# font_add("Noto Sans CJK TC", "/usr/share/fonts/opentype/noto/NotoSansCJK-Light.ttc")
font_add("Noto Sans CJK TC", "/notebooks/exploratory/NotoSansCJK-Light.ttc")

# 啟用 showtext_auto() 自動渲染文字
showtext_auto()

```

# Function

```{r}
# Function

# Read predict label data
Read_predict_label <- function(risklevel, refSentence, allFilesName_vec, foldpath){
  filteredFilesName <- allFilesName_vec[grep(risklevel, allFilesName_vec)]
  filteredFilesName <- filteredFilesName[grep(refSentence, filteredFilesName)]
  filteredFilesName <- filteredFilesName[grep("predict_label", filteredFilesName)]
  # It should be only one file
  input_tib <- read_csv(paste0(foldpath, "/", filteredFilesName))
  return(input_tib)
}
# Read predict metric data
Read_predict_metric <- function(risklevel, refSentence, allFilesName_vec, foldpath){
  filteredFilesName <- allFilesName_vec[grep(risklevel, allFilesName_vec)]
  filteredFilesName <- filteredFilesName[grep(refSentence, filteredFilesName)]
  filteredFilesName <- filteredFilesName[grep("predict_metric", filteredFilesName)]
  # It should be only one file
  input_tib <- read_csv(paste0(foldpath, "/", filteredFilesName))
  return(input_tib)
}


# function to plot metric data (Accuracy, Recall, Precision, F1 Score)
Plot_metric <- function(metric_tib, title, risklevel){
  level_num <- sub("risk", "", risklevel) %>% as.integer()
  title <- sprintf("%s (%s Risk Level)", title, level_num)
  ggplot(metric_tib, aes(x = cum_ut_i)) + 
    geom_line(aes(y = accuracy, colour = "Accuracy", linetype = "Accuracy")) +
    geom_line(aes(y = recall, colour = "Recall", linetype = "Recall")) +
    geom_line(aes(y = precision, colour = "Precision", linetype = "Precision")) +
    geom_line(aes(y = f1, colour = "F1 Score", linetype = "F1 Score")) +
    geom_hline(yintercept = (1 / level_num), linetype = "dashed", color = "black") +
    labs(x = "Cumulative Utterance Number", y = "Metrics Score", title = title) +
    theme_minimal() +
    theme(legend.title = element_blank()) +
    scale_y_continuous(limits = c(0, 0.7), breaks = seq(0, 0.6, 0.2)) +
    scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200)) +
    scale_colour_manual(
      name = "Metrics",   # 合併後的圖例名稱
      values = c(
        "Accuracy" = "#377eb8",
        "Recall" = "#4daf4a",
        "Precision" = "#e41a1c",
        "F1 Score" = "#984ea3"
      )
    ) +
    scale_linetype_manual(
      name = "Metrics",   # 合併後的圖例名稱
      values = c(
        "Accuracy" = "solid",
        "Recall" = "twodash",
        "Precision" = "dotted",
        "F1 Score" = "dotdash"
      )
    )
}
# function to plot metric data (graded precision, graded recall, FScore)
Plot_graded_metric <- function(metric_tib, title, risklevel){
  level_num <- sub("risk", "", risklevel) %>% as.integer()
  title <- sprintf("%s (%s Risk Level)", title, level_num)
  ggplot(metric_tib, aes(x = cum_ut_i)) + 
    geom_line(aes(y = grade_precision, colour = "Graded Precision", linetype = "Graded Precision")) +
    geom_line(aes(y = grade_recall, colour = "Graded Recall", linetype = "Graded Recall")) +
    geom_line(aes(y = FScore, colour = "F1 Score", linetype = "F1 Score")) +
    # geom_hline(yintercept = (1 / level_num), linetype = "dashed", color = "black") +
    labs(x = "Cumulative Utterance Number", y = "Metrics Score", title = title) +
    theme_minimal() +
    theme(legend.title = element_blank()) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
    scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200)) +
    scale_colour_manual(
      name = "Metrics",   # 合併後的圖例名稱
      values = c(
        "Graded Precision" = "#377eb8",
        "Graded Recall" = "#4daf4a",
        "F1 Score" = "#e41a1c"
      )
    ) +
    scale_linetype_manual(
      name = "Metrics",   # 合併後的圖例名稱
      values = c(
        "Graded Precision" = "solid",
        "Graded Recall" = "twodash",
        "F1 Score" = "dotdash"
      )
    )
}

# Calculate accuracy
CalcAccuracy <- function(df, true_col, pred_col) {
  true_values <- df[[true_col]]
  predicted_values <- df[[pred_col]]
  
  accuracy <- sum(true_values == predicted_values) / length(true_values)
  return(accuracy)
}
CalcAccuracy_grouped <- function(df_grouped, true_col, pred_col) {
  df_grouped %>%
    summarise(
      accuracy = list(CalcAccuracy(cur_data(), true_col, pred_col))
    ) %>%
    unnest(cols = c(accuracy))
}

# Calculate precision, recall, F1 score
CalcPrecision <- function(df, true_col, pred_col, average = "macro") {
  true_labels <- df %>% pull(true_col)
  predicted_labels <- df %>% pull(pred_col)

  conf_matrix <- table(predicted_labels, true_labels)
  classes <- sort(unique(c(true_labels, predicted_labels))) # 必須考慮全部類別
  # classes <- levels(factor(true_labels))
  
  precision_per_class <- sapply(classes, function(class) {
    tp <- sum(predicted_labels == class & true_labels == class)
    fp <- sum(predicted_labels == class & true_labels != class)
    if (tp + fp == 0) return(0) else return(tp / (tp + fp))
  })

  if (average == "macro") {
    return(mean(precision_per_class))
  } else if (average == "micro") {
    total_tp = sum(precision_per_class * table(predicted_labels))
    total_pred = length(predicted_labels)
    return(total_tp / total_pred)
  } else if (average == "weighted") {
    class_weights = table(true_labels) / length(true_labels)
    return(sum(precision_per_class * class_weights[classes]))
  } else {
    stop("Unknown average method")
  }
}


CalcRecall <- function(df, true_col, pred_col, average = "macro") {
  true_labels <- df %>% pull(true_col)
  predicted_labels <- df %>% pull(pred_col)
  
  conf_matrix <- table(predicted_labels, true_labels)
  classes <- sort(unique(c(true_labels, predicted_labels))) # 必須考慮全部類別
  # classes <- levels(factor(true_labels))  
  
  recall_per_class <- sapply(classes, function(class) {
    tp <- sum(predicted_labels == class & true_labels == class)
    fn <- sum(predicted_labels != class & true_labels == class)
    if (tp + fn == 0) return(0) else return(tp / (tp + fn))
  })
  
  if (average == "macro") {
    return(mean(recall_per_class))
  } else if (average == "micro") {
    total_tp = sum(recall_per_class * table(true_labels))
    total_actual = length(true_labels)
    return(total_tp / total_actual)
  } else if (average == "weighted") {
    class_weights = table(true_labels) / length(true_labels)
    return(sum(recall_per_class * class_weights[classes]))
  } else {
    stop("Unknown average method")
  }
}
CalcF1score <- function(df, true_col, pred_col, average = "macro") {
  # 直接使用修改后的 CalcPrecision 和 CalcRecall 函数
  precision <- CalcPrecision(df, true_col, pred_col, average)
  recall <- CalcRecall(df, true_col, pred_col, average)
  
  # 计算 F1 分数
  if (precision + recall == 0) {
    return(0)
  } else {
    f1_score <- 2 * (precision * recall) / (precision + recall)
    return(f1_score)
  }
}
# Work when group by
CalcPrecision_grouped <- function(df_grouped, true_col, pred_col, average = "macro") {
  df_grouped %>%
    summarise(
      precision = list(CalcPrecision(cur_data(), true_col, pred_col, average))
    ) %>%
    unnest(cols = c(precision))
}
CalcRecall_grouped <- function(df_grouped, true_col, pred_col, average = "macro") {
  df_grouped %>%
    summarise(
      recall = list(CalcRecall(cur_data(), true_col, pred_col, average))
    ) %>%
    unnest(cols = c(recall))
}
CalcF1score_grouped <- function(df_grouped, true_col, pred_col, average = "macro") {
  df_grouped %>%
    summarise(
      f1_score = list(CalcF1score(cur_data(), true_col, pred_col, average))
    ) %>%
    unnest(cols = c(f1_score))
}




# Calculate graded precision, recall, F1 score
CalcGradedPrecision <- function(df, true_col, pred_col) {
  y_true_int <- df[[true_col]] %>% as.integer()
  y_pred_int <- df[[pred_col]] %>% as.integer()
  
  new_fp <- sum(sapply(1:length(y_true_int), function(i) y_true_int[i] < y_pred_int[i]))
  new_tp <- sum(sapply(1:length(y_true_int), function(i) y_true_int[i] == y_pred_int[i]))
  
  graded_precision <- new_tp / (new_tp + new_fp)
  return(graded_precision)
}
CalcGradedRecall <- function(df, true_col, pred_col) {
  y_true_int <- df[[true_col]] %>% as.integer()
  y_pred_int <- df[[pred_col]] %>% as.integer()
  
  new_fn <- sum(sapply(1:length(y_true_int), function(i) y_true_int[i] > y_pred_int[i]))
  new_tp <- sum(sapply(1:length(y_true_int), function(i) y_true_int[i] == y_pred_int[i]))
  
  graded_recall <- new_tp / (new_tp + new_fn)
  return(graded_recall)
}
CalcFScore <- function(df, true_col, pred_col) {
  precision <- CalcGradedPrecision(df, true_col, pred_col)
  recall <- CalcGradedRecall(df, true_col, pred_col)
  FScore <- 2 * precision * recall / (precision + recall)
  return(FScore)
}
# Work when group by
CalcGradedPrecision_grouped <- function(df_grouped, true_col, pred_col) {
  df_grouped %>%
    summarise(graded_precision = CalcGradedPrecision(cur_data(), true_col, pred_col)) %>% 
    unnest(cols = c(graded_precision))
}
CalcGradedRecall_grouped <- function(df_grouped, true_col, pred_col) {
  df_grouped %>%
    summarise(graded_recall = CalcGradedRecall(cur_data(), true_col, pred_col)) %>% 
    unnest(cols = c(graded_recall))
}
CalcFScore_grouped <- function(df_grouped, true_col, pred_col) {
  df_grouped %>%
    summarise(FScore = CalcFScore(cur_data(), true_col, pred_col)) %>% 
    unnest(cols = c(FScore))
}


# Calculate all metrics
CalcAllScores <- function(df, true_col, pred_col, average = "macro") {
  # 計算基礎指標
  accuracy <- CalcAccuracy(df, true_col, pred_col)
  precision <- CalcPrecision(df, true_col, pred_col, average)
  recall <- CalcRecall(df, true_col, pred_col, average)
  f1_score <- CalcF1score(df, true_col, pred_col, average)

  # 計算分級指標
  graded_precision <- CalcGradedPrecision(df, true_col, pred_col)
  graded_recall <- CalcGradedRecall(df, true_col, pred_col)
  fscore <- CalcFScore(df, true_col, pred_col)

  # 返回所有指標
  return(list(
    accuracy = accuracy,
    precision = precision, 
    recall = recall, 
    f1_score = f1_score, 
    graded_precision = graded_precision, 
    graded_recall = graded_recall,
    fscore = fscore
  ))
}
# Work when group by
CalcAllScores_grouped <- function(df_grouped, true_col, pred_col, average = "macro") {
  df_grouped %>%
    summarise(
      accuracy = CalcAccuracy_grouped(cur_data(), true_col, pred_col),
      precision = CalcPrecision_grouped(cur_data(), true_col, pred_col, average),
      recall = CalcRecall_grouped(cur_data(), true_col, pred_col, average),
      f1_score = CalcF1score_grouped(cur_data(), true_col, pred_col, average),
      graded_precision = CalcGradedPrecision_grouped(cur_data(), true_col, pred_col),
      graded_recall = CalcGradedRecall_grouped(cur_data(), true_col, pred_col),
      FScore = CalcFScore_grouped(cur_data(), true_col, pred_col)
    ) %>%
    unnest(cols = c(accuracy, precision, recall, f1_score, graded_precision, graded_recall, FScore))
}

# Calculate metrics that don't need average
CalcNonAvgScores_grouped <- function(df_grouped, true_col, pred_col) {
  df_grouped %>%
    summarise(
      accuracy = CalcAccuracy_grouped(cur_data(), true_col, pred_col),
      graded_precision = CalcGradedPrecision_grouped(cur_data(), true_col, pred_col),
      graded_recall = CalcGradedRecall_grouped(cur_data(), true_col, pred_col),
      FScore = CalcFScore_grouped(cur_data(), true_col, pred_col)
    ) %>%
    unnest(cols = c(accuracy, graded_precision, graded_recall, FScore))
}

```



```{r}
# Set folder path
processed_path <- "data/processed/"
rfReports_path <- "results/rf_cum_utterance"
bertReports_path <- "results/bert_cum_utterance"

rf_allFilesName_vec <- list.files(rfReports_path, pattern = "\\.csv$", full.names = TRUE) %>% basename()
bert_allFilesName_vec <- list.files(bertReports_path, pattern = "\\.csv$", full.names = TRUE) %>% basename()

```

```{r include=FALSE}
# read transcript and risklevel data
risklevel_tib <- read_csv(paste0(processed_path, "risklevel.csv"))
transcript_caller_tib <- read_csv(paste0(processed_path, "transcripts_emb_caller.csv"))
transcript_responser_tib <- read_csv(paste0(processed_path, "transcripts_emb_responser.csv"))

# Count utterances by each dialogue_id
utNum_caller_tib <- transcript_caller_tib %>% 
  group_by(dialogue_id) %>%
  summarise(utNum_caller = n())
utNum_responser_tib <- transcript_responser_tib %>% 
  group_by(dialogue_id) %>%
  summarise(utNum_responser = n())

# join by dialogue_id
utNum_tib <- utNum_caller_tib %>% 
  left_join(., utNum_responser_tib, by = "dialogue_id")
data_tib <- risklevel_tib %>% 
  left_join(., utNum_tib, by = "dialogue_id")

# Group transcripts by utterance number of caller
cut_off1 <- 150
cut_off2 <- 300
data_tib <- data_tib %>% 
  mutate(utNum_caller_group = case_when(
    utNum_caller <= cut_off1 ~ "short",
    utNum_caller > cut_off1 & utNum_caller <= cut_off2 ~ "medium",
    utNum_caller > cut_off2 ~ "long"
  ))
# Group transcripts by utterance number of responser
cut_off1 <- 175
cut_off2 <- 350
data_tib <- data_tib %>% 
  mutate(utNum_responser_group = case_when(
    utNum_responser <= cut_off1 ~ "short",
    utNum_responser > cut_off1 & utNum_responser <= cut_off2 ~ "medium",
    utNum_responser > cut_off2 ~ "long"
  ))

groupDialogue_tib <- data_tib %>% 
  select(dialogue_id, utNum_caller_group, utNum_responser_group)
```

# Risk5 X RF Model

```{r include=FALSE}
risklevel <- "risk5"
refSentence <- "max_sim_ref3"
# read cumulative utterance predicted label data
rf_label_tib <- Read_predict_label(risklevel, refSentence, rf_allFilesName_vec, rfReports_path)
```

```{r}
# 計算分組的所有指標分數
groupedAllScores_tib <- rf_label_tib %>%
  left_join(., groupDialogue_tib, by = "dialogue_id") %>%
  # as.factor()
  mutate(!!sym(risklevel)  := !!sym(risklevel) %>% as.factor()) %>%
  # y_pred 的 level 設為跟 risklevel 一樣
  mutate(y_pred = y_pred %>% factor(levels = levels(!!sym(risklevel)))) %>%
  mutate(risk_group = !!sym(risklevel) %>% as.factor()) %>%
  mutate(utNum_group = utNum_caller_group) %>%
  group_by(utNum_group, cum_ut) %>%
  CalcAllScores_grouped(
    ., 
    true_col = risklevel,
    pred_col = "y_pred"
  )
# 將 有 na 的元素補值為 0
# groupedAllScores_tib <- groupedAllScores_tib %>% 
#   mutate(across(where(is.numeric), ~replace(., is.na(.), 0)))
```

```{r}
# plot accuracy, recall, precision, F1 score grouped by numbers of utterance
g1 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = accuracy, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Accuracy", title = "Accuracy") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g2 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Precision", title = "Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g3 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Recall", title = "Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g4 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = f1_score, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "F1 Score", title = "F1 Score") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2, 
                        top = "RF Model Metrics Over Cumulative Utterances (Risk5)"
                        )
```

```{r}
# plot graded precision, recall, FScore grouped by numbers of utterance
g5 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = graded_precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Precision", title = "Graded Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g6 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = graded_recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Recall", title = "Graded Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g7 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = FScore, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "FScore", title = "FScore") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g5, g6, g7, ncol = 2, 
                        top = "RF Model Graded Metrics Over Cumulative Utterances (Risk5)"
                        )
```



# Risk5 X BERT Model

```{r include=FALSE}
# read cumulative utterance prediction data
risklevel <- "risk5"
refSentence <- "max_sim_ref2"
# read cumulative utterance predicted label data
bert_label_tib <- Read_predict_label(risklevel, refSentence, bert_allFilesName_vec, bertReports_path)
```

```{r include=FALSE}
# 計算分組的所有指標分數
groupedAllScores_tib <- bert_label_tib %>%
  left_join(., groupDialogue_tib, by = "dialogue_id") %>%
  # as.factor()
  mutate(!!sym(risklevel)  := !!sym(risklevel) %>% as.factor()) %>%
  # y_pred 的 level 設為跟 risklevel 一樣
  mutate(y_pred = y_pred %>% factor(levels = levels(!!sym(risklevel)))) %>%
  mutate(risk_group = !!sym(risklevel) %>% as.factor()) %>%
  mutate(utNum_group = utNum_caller_group) %>%
  group_by(utNum_group, cum_ut) %>%
  CalcAllScores_grouped(
    ., 
    true_col = risklevel,
    pred_col = "y_pred"
  )

# 將有 na 的元素補值為 0
# groupedAllScores_tib <- groupedAllScores_tib %>% 
#   mutate(across(where(is.numeric), ~replace(., is.na(.), 0)))
```

```{r}
# plot accuracy, grouped precision, recall, FScore by risk level
g1 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = accuracy, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "Accuracy", title = "Accuracy") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g2 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = precision, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "Precision", title = "Precision") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g3 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = recall, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "Recall", title = "Recall") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g4 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = f1_score, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "F1 Score", title = "F1 Score") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2, 
                        top = "BERT Model Metrics Over Cumulative Utterances (Risk5)"
                        )
```

```{r}
# plot graded precision, recall, FScore grouped by numbers of utterance
g5 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = graded_precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Precision", title = "Graded Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g6 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = graded_recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Recall", title = "Graded Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g7 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = FScore, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "FScore", title = "FScore") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g5, g6, g7, ncol = 2, 
                        top = "BERT Model Graded Metrics Over Cumulative Utterances (Risk5)"
                        )
```

# Risk4 X RF Model

```{r include=FALSE}
risklevel <- "risk4"
refSentence <- "avg_sim_ref3"
# read cumulative utterance predicted label data
rf_label_tib <- Read_predict_label(risklevel, refSentence, rf_allFilesName_vec, rfReports_path)
```

```{r}
# 計算分組的所有指標分數
groupedAllScores_tib <- rf_label_tib %>% 
  left_join(., groupDialogue_tib, by = "dialogue_id") %>%
  mutate(risk_group = !!sym(risklevel) %>% as.factor()) %>%
  mutate(utNum_group = utNum_caller_group) %>%
  group_by(utNum_group, cum_ut) %>%
  CalcAllScores_grouped(
    ., 
    true_col = risklevel,
    pred_col = "y_pred",
  )
```

```{r}
# plot accuracy, recall, precision, F1 score grouped by numbers of utterance
g1 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = accuracy, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Accuracy", title = "Accuracy") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g2 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Precision", title = "Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g3 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Recall", title = "Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g4 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = f1_score, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "F1 Score", title = "F1 Score") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2, 
                        top = "RF Model Metrics Over Cumulative Utterances (Risk4)"
                        )
```

```{r}
# plot graded precision, recall, FScore grouped by numbers of utterance
g5 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = graded_precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Precision", title = "Graded Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g6 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = graded_recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Recall", title = "Graded Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g7 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = FScore, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "FScore", title = "FScore") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g5, g6, g7, ncol = 2, 
                        top = "RF Model Graded Metrics Over Cumulative Utterances (Risk4)"
                        )
```


# Risk4 X BERT Model

因為分組後 y_pred 缺少部份類別，所以須平均的指標算不出來

```{r include=FALSE}
risklevel <- "risk4"
refSentence <- "avg_sim_ref2"
# read cumulative utterance predicted label data
bert_label_tib <- Read_predict_label(risklevel, refSentence, bert_allFilesName_vec, bertReports_path)
```

```{r}
# 計算分組的不需要平均的指標分數
groupedAllScores_tib <- bert_label_tib %>%
  left_join(., groupDialogue_tib, by = "dialogue_id") %>%
  # as.factor()
  mutate(!!sym(risklevel)  := !!sym(risklevel) %>% as.factor()) %>%
  # y_pred 的 level 設為跟 risklevel 一樣
  mutate(y_pred = y_pred %>% factor(levels = levels(!!sym(risklevel)))) %>%
  mutate(risk_group = !!sym(risklevel) %>% as.factor()) %>%
  mutate(utNum_group = utNum_caller_group) %>%
  group_by(utNum_group, cum_ut) %>%
  CalcAllScores_grouped(
    ., 
    true_col = risklevel,
    pred_col = "y_pred"
  )

# 將有 na 的元素補值為 0
# groupedAllScores_tib <- groupedAllScores_tib %>% 
#   mutate(across(where(is.numeric), ~replace(., is.na(.), 0)))
```

```{r}
# plot accuracy, grouped precision, recall, FScore by risk level
g1 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = accuracy, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "Accuracy", title = "Accuracy") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g2 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = precision, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "Precision", title = "Precision") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g3 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = recall, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "Recall", title = "Recall") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g4 <- groupedAllScores_tib %>%
 ggplot(aes(x = cum_ut, y = f1_score, group = utNum_group, color = utNum_group)) +
 geom_line() +
 labs(x = "Cumulative Utterance Number", y = "F1 Score", title = "F1 Score") +
 theme_minimal() +
 # theme(legend.position = "none") +
 scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
 scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2, 
                        top = "BERT Model Metrics Over Cumulative Utterances (Risk4)"
                        )
```

```{r}
# plot graded precision, recall, FScore grouped by numbers of utterance
g5 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = graded_precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Precision", title = "Graded Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g6 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = graded_recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Recall", title = "Graded Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g7 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = FScore, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "FScore", title = "FScore") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g5, g6, g7, ncol = 2, 
                        top = "BERT Model Graded Metrics Over Cumulative Utterances (Risk4)"
                        )  
```



# Risk3 X RF Model

```{r include=FALSE}
risklevel <- "risk3"
refSentence <- "max_sim_ref3"
# read cumulative utterance predicted label data
rf_label_tib <- Read_predict_label(risklevel, refSentence, rf_allFilesName_vec, rfReports_path)
```


```{r include=FALSE}
# 計算分組的所有指標分數
groupedAllScores_tib <- rf_label_tib %>% 
  left_join(., groupDialogue_tib, by = "dialogue_id") %>%
  mutate(risk_group = !!sym(risklevel) %>% as.factor()) %>%
  mutate(utNum_group = utNum_caller_group) %>%
  group_by(utNum_group, cum_ut) %>%
  CalcAllScores_grouped(
    ., 
    true_col = risklevel,
    pred_col = "y_pred"
  )
```

```{r}
# plot accuracy, recall, precision, F1 score grouped by numbers of utterance
g1 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = accuracy, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Accuracy", title = "Accuracy") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g2 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Precision", title = "Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g3 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Recall", title = "Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g4 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = f1_score, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "F1 Score", title = "F1 Score") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2, 
                        top = "RF Model Metrics Over Cumulative Utterances (Risk3)"
                        )
```

```{r}
# plot graded precision, recall, FScore grouped by numbers of utterance
g5 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = graded_precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Precision", title = "Graded Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g6 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = graded_recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Recall", title = "Graded Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g7 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = FScore, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "FScore", title = "FScore") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g5, g6, g7, ncol = 2, 
                        top = "RF Model Graded Metrics Over Cumulative Utterances (Risk3)"
                        )
```


# Risk3 X BERT Model

```{r include=FALSE}
risklevel <- "risk3"
refSentence <- "max_sim_ref1"
# read cumulative utterance predicted label data
bert_label_tib <- Read_predict_label(risklevel, refSentence, bert_allFilesName_vec, bertReports_path)
```


```{r}
# 計算分組的所有指標分數
groupedAllScores_tib <- bert_label_tib %>% 
  left_join(., groupDialogue_tib, by = "dialogue_id") %>%
  mutate(risk_group = !!sym(risklevel) %>% as.factor()) %>%
  mutate(utNum_group = utNum_caller_group) %>%
  group_by(utNum_group, cum_ut) %>%
  CalcAllScores_grouped(
    ., 
    true_col = risklevel,
    pred_col = "y_pred"
  )
```

```{r}
# plot accuracy, recall, precision, F1 score grouped by numbers of utterance
g1 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = accuracy, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number ", y = "Accuracy", title = "Accuracy") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g2 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Precision", title = "Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g3 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Recall", title = "Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g4 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = f1_score, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "F1 Score", title = "F1 Score") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2, 
                        top = "BERT Model Metrics Over Cumulative Utterances (Risk3)"
                        )
```

```{r}
# plot graded precision, recall, FScore grouped by numbers of utterance
g5 <- groupedAllScores_tib %>% 
  ggplot(aes(x = cum_ut, y = graded_precision, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Precision", title = "Graded Precision") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g6 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = graded_recall, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "Graded Recall", title = "Graded Recall") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))
g7 <- groupedAllScores_tib %>%
  ggplot(aes(x = cum_ut, y = FScore, group = utNum_group, color = utNum_group)) +
  geom_line() +
  labs(x = "Cumulative Utterance Number", y = "FScore", title = "FScore") +
  theme_minimal() +
  # theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +
  scale_x_continuous(limits = c(0, NA), breaks = seq(0, 1000, 200))

gridExtra::grid.arrange(g5, g6, g7, ncol = 2, 
                        top = "BERT Model Graded Metrics Over Cumulative Utterances (Risk3)"
                        )
```

